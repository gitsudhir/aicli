# RAG Pipeline (Rust, Ollama + Qdrant)

This folder contains a Rust-only, local RAG pipeline that:
1. Scans files
2. Chunks text
3. Embeds chunks with Ollama
4. Stores vectors in Qdrant
5. Embeds the user query
6. Retrieves top chunks
7. Builds a structured prompt
8. Generates an answer with Ollama

## Prerequisites
- Ollama running at `http://localhost:11434`
- Qdrant running at `http://localhost:6333`
- Rust toolchain

## Use In App
This RAG crate is wired into the main TUI app. In `Text` mode, your prompt is enhanced with retrieved context and the answer is generated by Ollama.

## Configuration (env vars)
- `RAG_SOURCE_DIR` (default `./`)
- `RAG_INCLUDE_EXTS` (default: .rs,.md,.txt,.toml,.json,.yaml,.yml,.py,.js,.ts,.tsx,.html,.css)
- `RAG_MAX_FILE_BYTES` (default `500000`)
- `RAG_EXCLUDE_DIRS` (default `.git,target,node_modules,.idea,.vscode,dist,build`)
- `RAG_CHUNK_SIZE` (default `1200`)
- `RAG_CHUNK_OVERLAP` (default `200`)
- `OLLAMA_URL` (default `http://localhost:11434`)
- `OLLAMA_EMBED_MODEL` (default `nomic-embed-text`)
- `OLLAMA_CHAT_MODEL` (default `qwen2.5-coder:14b`)
- `QDRANT_URL` (default `http://localhost:6333`)
- `QDRANT_COLLECTION` (default `rag_chunks`)
- `QDRANT_DISTANCE` (default `Cosine`)
- `RAG_TOP_K` (default `5`)
- `RAG_SYSTEM_PROMPT` (default: "You are a helpful coding assistant. Use only the provided context.")

## Files
- `rag/src/scan_files.rs` — scan the filesystem
- `rag/src/chunk_text.rs` — chunking logic
- `rag/src/embed_chunks.rs` — embeddings via Ollama
- `rag/src/store_qdrant.rs` — create collection and store vectors
- `rag/src/embed_query.rs` — embed user query
- `rag/src/retrieve_chunks.rs` — retrieve top chunks from Qdrant
- `rag/src/build_prompt.rs` — build a structured prompt
- `rag/src/generate.rs` — generate answer with Ollama

## Notes
- Embeddings use `/api/embed` with fallback to `/api/embeddings`.
- Generation uses `/api/chat` with your Qwen model.
