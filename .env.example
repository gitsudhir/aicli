# Core model endpoints
OLLAMA_URL=http://localhost:11434
OLLAMA_EMBED_MODEL=nomic-embed-text
OLLAMA_CHAT_MODEL=qwen2.5-coder:14b

# Qdrant
QDRANT_URL=http://localhost:6333
# QDRANT_COLLECTION=aicli_rag_chunks
# QDRANT_DISTANCE=Cosine

# RAG scan/chunk behavior
RAG_SOURCE_DIR=./
RAG_INCLUDE_EXTS=.rs,.md,.txt,.toml,.json,.yaml,.yml,.py,.js,.ts,.tsx,.html,.css
RAG_EXCLUDE_DIRS=.git,target,node_modules,.idea,.vscode,dist,build
RAG_MAX_FILE_BYTES=500000
RAG_CHUNK_SIZE=1200
RAG_CHUNK_OVERLAP=200
RAG_TOP_K=5

# Classic RAG prompt
RAG_SYSTEM_PROMPT=You are a helpful coding assistant. Use only the provided context.

# Hybrid agent controls
RAG_AGENT_MAX_STEPS=10
RAG_HYBRID_SYSTEM_PROMPT=You are a hybrid AI agent. You can retrieve knowledge, call MCP tools, fetch MCP prompts, read MCP resources, or answer directly. Always return valid JSON with one action: retrieve | tool | prompt | resource | final.

# MCP transport (HTTP preferred)
# Set MCP_URL for JSON-RPC over HTTP:
# MCP_URL=http://localhost:8080/mcp
MCP_URL=

# Stdio fallback when MCP_URL is empty:
# MCP_COMMAND=/absolute/path/to/mcp-server-binary
# MCP_ARGS=
#
# Example for sudhir's local Rust MCP server:
# MCP_COMMAND=/Users/sudhirkumar/Desktop/sudhir/gitsudhir/mcp-server-rust/target/release/mcp-server-rust
# MCP_ARGS=
MCP_COMMAND=
MCP_ARGS=
